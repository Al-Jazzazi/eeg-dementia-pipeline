{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O4Yf-IoK0js"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install PyWavelets\n",
    "!pip install mne\n",
    "!pip install scikit-image\n",
    "!pip install -U ipywidgets\n",
    "!pip install \"ray[tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqS6KVS9K0jo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import mne\n",
    "import re\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from typing import Optional, Dict, Union, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import  confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time # To measure time\n",
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_eeg_data(folder_path):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".set\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "            data[filename] = raw.get_data()\n",
    "    return data\n",
    "\n",
    "def read_json_dicts(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data_dict = json.load(f)\n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH5v5FIEK0jt"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eegnet_minimal(eeg_data, fs, lowcut=1.0, highcut=40.0, order=5):\n",
    "    \"\"\"\n",
    "    Applies minimal preprocessing suitable for models like EEGNet:\n",
    "    Bandpass filtering and channel-wise standardization.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        lowcut (float): Lower cutoff frequency for bandpass filter (Hz).\n",
    "        highcut (float): Upper cutoff frequency for bandpass filter (Hz).\n",
    "        order (int): Order of the Butterworth filter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed EEG data (n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    processed_data = np.zeros_like(eeg_data)\n",
    "\n",
    "    # 1. Bandpass Filter Design\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    # Ensure frequency bounds are valid\n",
    "    if low <= 0 or high >= 1:\n",
    "         print(f\"Warning: Filter frequencies ({lowcut}Hz, {highcut}Hz) are invalid for Nyquist freq {nyq}Hz. Adjusting or skipping filter.\")\n",
    "         # Option: Skip filtering or adjust bounds\n",
    "         b, a = None, None # Indicate filter skip\n",
    "    else:\n",
    "        try:\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not design Butterworth filter (order={order}, freqs=[{low}, {high}]). Skipping filter. Error: {e}\")\n",
    "            b, a = None, None\n",
    "\n",
    "\n",
    "    # 2. Apply Filter and Standardize Channel by Channel\n",
    "    for i_ch in range(n_channels):\n",
    "        channel_data = eeg_data[i_ch, :]\n",
    "\n",
    "        # Apply filtering if filter design was successful\n",
    "        if b is not None and a is not None:\n",
    "             try:\n",
    "                 filtered_data = filtfilt(b, a, channel_data)\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Filtering failed for channel {i_ch}. Using original data for this channel. Error: {e}\")\n",
    "                 filtered_data = channel_data # Use original if filtering fails\n",
    "        else:\n",
    "             filtered_data = channel_data # Use original if filter wasn't designed\n",
    "\n",
    "        # Standardize (z-score normalization)\n",
    "        mean = np.mean(filtered_data)\n",
    "        std = np.std(filtered_data)\n",
    "        if std > 1e-9: # Avoid division by zero\n",
    "            processed_data[i_ch, :] = (filtered_data - mean) / std\n",
    "        else:\n",
    "            processed_data[i_ch, :] = filtered_data - mean # Only center if std is zero\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aYbQDIy8K0ju"
   },
   "outputs": [],
   "source": [
    "def calculate_td_psd_features(epoch_data, fs, power_lambda=0.1, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculates the 7 TD-PSD features for a single EEG epoch (single channel).\n",
    "    Based on equations in Amini et al., 2021.\n",
    "\n",
    "    Args:\n",
    "        epoch_data (np.ndarray): 1D numpy array for a single channel epoch.\n",
    "        fs (float): Sampling frequency of the epoch data.\n",
    "        power_lambda (float): Lambda for power transform normalization.\n",
    "        epsilon (float): Small value to prevent log(0) or division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the 7 log-transformed TD-PSD features.\n",
    "                    Returns NaNs if calculation fails.\n",
    "    \"\"\"\n",
    "    n_samples = len(epoch_data)\n",
    "    if n_samples == 0:\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    # Detrend the signal (optional but often good practice)\n",
    "    signal = detrend(epoch_data)\n",
    "\n",
    "    # 1. Calculate Power Spectrum and Moments\n",
    "    try:\n",
    "        # FFT\n",
    "        X = fft(signal)\n",
    "        # Power Spectrum (One-sided, ignoring DC for moments perhaps?)\n",
    "        # Frequencies for moments k: corresponds to frequency bins\n",
    "        freqs = np.fft.fftfreq(n_samples, 1/fs)\n",
    "        # Power spectrum P[k] = |X[k]|^2 / N\n",
    "        P = np.abs(X)**2 / n_samples\n",
    "\n",
    "        # Calculate moments m0, m2, m4\n",
    "        # m_n = sum(f^n * P(f)) df - approximated by sum(k^n * P[k])\n",
    "        # We use the magnitude of frequencies for k, ignore negative freqs?\n",
    "        # Let's use Hjorth parameters definition based on time-domain variance\n",
    "        # m0 = variance(signal) = total power (approx)\n",
    "        m0_bar = np.sum(signal**2) / n_samples # Variance = mean square if mean is zero\n",
    "        if m0_bar < epsilon: m0_bar = epsilon\n",
    "\n",
    "        # m2 = variance of first derivative (activity)\n",
    "        delta_x = np.diff(signal, n=1) * fs # Scale by fs? Hjorth doesn't explicitly scale by fs\n",
    "        m2_bar = np.sum(delta_x**2) / (n_samples -1) # Use n_samples-1?\n",
    "        if m2_bar < epsilon: m2_bar = epsilon\n",
    "\n",
    "\n",
    "        # m4 = variance of second derivative (mobility)\n",
    "        delta2_x = np.diff(signal, n=2) * (fs**2) # Scale by fs^2?\n",
    "        m4_bar = np.sum(delta2_x**2) / (n_samples -2)\n",
    "        if m4_bar < epsilon: m4_bar = epsilon\n",
    "\n",
    "\n",
    "        # Apply power transform (Box-Cox with lambda=0 is log, this is slightly different)\n",
    "        m0 = (m0_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m0_bar)\n",
    "        m2 = (m2_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m2_bar)\n",
    "        m4 = (m4_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m4_bar)\n",
    "\n",
    "        # Ensure moments are positive after transform for log\n",
    "        m0 = max(m0, epsilon)\n",
    "        m2 = max(m2, epsilon)\n",
    "        m4 = max(m4, epsilon)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating moments: {e}\")\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    features = np.zeros(7)\n",
    "\n",
    "    # 2. Calculate Features f1, f2, f3\n",
    "    try:\n",
    "        features[0] = np.log(m0) # f1 = log(m0)\n",
    "        # Check for valid subtractions\n",
    "        if m0 <= m2: m0 = m2 + epsilon\n",
    "        if m0 <= m4: m0 = m4 + epsilon\n",
    "        features[1] = np.log(m0 - m2) # f2 = log(m0 - m2)\n",
    "        features[2] = np.log(m0 - m4) # f3 = log(m0 - m4)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f1, f2, f3: {e}\")\n",
    "         features[:3] = np.nan\n",
    "\n",
    "\n",
    "    # 3. Calculate Feature f4 (Sparseness)\n",
    "    try:\n",
    "        denominator_sqrt = np.sqrt(max(m0 - m2, epsilon)) * np.sqrt(max(m0 - m4, epsilon))\n",
    "        if denominator_sqrt < epsilon: denominator_sqrt = epsilon\n",
    "        features[3] = np.log(m0 / denominator_sqrt) # f4 = log(S) = log(m0 / sqrt((m0-m2)(m0-m4)))\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f4 (Sparseness): {e}\")\n",
    "         features[3] = np.nan\n",
    "\n",
    "    # 4. Calculate Feature f5 (Irregularity Factor - IF)\n",
    "    # IF = (m4/m2) / (m2/m0) based on Hjorth parameters 'complexity'\n",
    "    # Paper formula: sqrt(m4/m2) / sqrt(m2/m0) => m0*m4 / m2^2\n",
    "    try:\n",
    "        if m2 < epsilon: m2 = epsilon\n",
    "        if_val = (m0 * m4) / (m2**2)\n",
    "        features[4] = np.log(max(if_val, epsilon)) # f5 = log(IF)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f5 (IF): {e}\")\n",
    "        features[4] = np.nan\n",
    "\n",
    "    # 5. Calculate Feature f6 (Covariance - COV)\n",
    "    # COV = std_dev / mean\n",
    "    try:\n",
    "        mean_val = np.mean(signal)\n",
    "        std_dev_val = np.std(signal)\n",
    "        if abs(mean_val) < epsilon: mean_val = np.sign(mean_val) * epsilon if mean_val != 0 else epsilon\n",
    "        cov_val = std_dev_val / mean_val\n",
    "        features[5] = np.log(max(abs(cov_val), epsilon)) # Log of magnitude? Paper isn't explicit if COV can be negative. Let's take abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f6 (COV): {e}\")\n",
    "        features[5] = np.nan\n",
    "\n",
    "\n",
    "    # 6. Calculate Feature f7 (Teager Energy Operator - TEO)\n",
    "    try:\n",
    "        # TEO(x[j]) = x[j]^2 - x[j-1]x[j+1]\n",
    "        # Need to handle boundaries (pad or slice)\n",
    "        teo_vals = signal[1:-1]**2 - signal[:-2] * signal[2:]\n",
    "        sum_teo = np.sum(teo_vals)\n",
    "        features[6] = np.log(max(abs(sum_teo), epsilon)) # Log of magnitude? Sum can be negative. Paper isn't explicit. Taking abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f7 (TEO): {e}\")\n",
    "        features[6] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_amini(eeg_data, fs, target_fs=256, target_duration_sec=180, epoch_len=256):\n",
    "    \"\"\"\n",
    "    Preprocesses raw EEG data according to Amini et al. (2021).\n",
    "    Selects time window, downsamples (if needed), segments, extracts TD-PSD features,\n",
    "    and averages features across epochs for each channel.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        target_fs (int): Target sampling frequency (default: 256 Hz).\n",
    "        target_duration_sec (int): Duration of the segment to analyze (default: 180s).\n",
    "        epoch_len (int): Length of epochs for feature calculation (default: 256 samples).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix (n_channels, 7), or None if error.\n",
    "                   Returns NaNs for channels/features where calculation failed.\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "\n",
    "    # 1. Select Time Window (e.g., 60s to 60s + target_duration_sec)\n",
    "    start_sample = int(60 * fs) # Example: start at 60 seconds\n",
    "    end_sample = start_sample + int(target_duration_sec * fs)\n",
    "    if end_sample > n_timesteps:\n",
    "        print(f\"Warning: Data duration ({n_timesteps/fs:.2f}s) is less than required analysis window ({target_duration_sec+60}s). Using available data.\")\n",
    "        end_sample = n_timesteps\n",
    "    if start_sample >= end_sample:\n",
    "         print(f\"Error: Invalid time window selection.\")\n",
    "         return None\n",
    "    eeg_segment = eeg_data[:, start_sample:end_sample]\n",
    "\n",
    "    # 2. Downsample (if necessary)\n",
    "    if fs != target_fs:\n",
    "        from scipy.signal import resample\n",
    "        num_samples_resampled = int(eeg_segment.shape[1] * (target_fs / fs))\n",
    "        try:\n",
    "            eeg_resampled = resample(eeg_segment, num_samples_resampled, axis=1)\n",
    "            current_fs = target_fs\n",
    "            print(f\"Resampled data from {fs}Hz to {target_fs}Hz. New shape: {eeg_resampled.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during resampling: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        eeg_resampled = eeg_segment\n",
    "        current_fs = fs\n",
    "\n",
    "    # 3. Segment into epochs and Calculate Features\n",
    "    n_channels_res, n_timesteps_res = eeg_resampled.shape\n",
    "    num_epochs = n_timesteps_res // epoch_len\n",
    "    if num_epochs == 0:\n",
    "        print(f\"Error: Resampled data is shorter ({n_timesteps_res} samples) than epoch length ({epoch_len}). Cannot extract features.\")\n",
    "        return None\n",
    "\n",
    "    all_channel_features = []\n",
    "    for i_ch in range(n_channels_res):\n",
    "        channel_data = eeg_resampled[i_ch, :]\n",
    "        epoch_features_list = []\n",
    "        for i_epoch in range(num_epochs):\n",
    "            start = i_epoch * epoch_len\n",
    "            end = start + epoch_len\n",
    "            epoch = channel_data[start:end]\n",
    "            features = calculate_td_psd_features(epoch, current_fs)\n",
    "            if not np.isnan(features).all(): # Only add if calculation didn't fail completely\n",
    "                epoch_features_list.append(features)\n",
    "\n",
    "        if not epoch_features_list: # If no features calculated for this channel\n",
    "             avg_features = np.full(7, np.nan)\n",
    "        else:\n",
    "            # Average features across valid epochs for the channel\n",
    "            avg_features = np.nanmean(np.array(epoch_features_list), axis=0)\n",
    "        all_channel_features.append(avg_features)\n",
    "\n",
    "    # Output shape: (n_channels, 7)\n",
    "    final_features = np.array(all_channel_features)\n",
    "    # Optional: Flatten to a single vector per subject if needed by the specific model implementation\n",
    "    # final_features = final_features.flatten() # Shape: (n_channels * 7)\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc1xIGdpK0jw"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5AixcgkscKr"
   },
   "outputs": [],
   "source": [
    "class Amini_Adapted_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted CNN model based on the structure described in Amini et al. (2021).\n",
    "    Original paper uses 7 TD-PSD features as input. This adaptation takes raw EEG\n",
    "    (n_channels, n_timesteps) as input using 1D convolutions.\n",
    "    Specific parameters (kernel sizes, filter numbers) are assumptions based on\n",
    "    common practices as they were not detailed for raw EEG in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes, dropout_rate=0.5):\n",
    "        super(Amini_Adapted_CNN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional Layer(s) - Example: One Conv1D layer\n",
    "        # Input shape: (batch_size, n_channels, n_timesteps)\n",
    "        # Output shape: (batch_size, out_channels, new_timesteps)\n",
    "        # Using a relatively large kernel for initial temporal feature extraction\n",
    "        self.conv1_out_channels = 16\n",
    "        self.conv1_kernel_size = 64 # Example kernel size\n",
    "        self.conv1_stride = 16      # Example stride for downsampling\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_channels,\n",
    "                               out_channels=self.conv1_out_channels,\n",
    "                               kernel_size=self.conv1_kernel_size,\n",
    "                               stride=self.conv1_stride)\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_out_channels)\n",
    "\n",
    "        # Calculate the output size after convolution and pooling (if any)\n",
    "        # Assuming no pooling here, just convolution\n",
    "        conv1_out_timesteps = (n_timesteps - self.conv1_kernel_size) // self.conv1_stride + 1\n",
    "        self.fc1_input_features = self.conv1_out_channels * conv1_out_timesteps\n",
    "\n",
    "        # Fully Connected Layers (mimicking Figure 10 structure)\n",
    "        # Paper shows 3 FC layers before Softmax/Classification\n",
    "        self.fc1_hidden_units = 128 # Example hidden units\n",
    "        self.fc2_hidden_units = 64  # Example hidden units\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc1_input_features, self.fc1_hidden_units)\n",
    "        self.fc2 = nn.Linear(self.fc1_hidden_units, self.fc2_hidden_units)\n",
    "        self.fc3 = nn.Linear(self.fc2_hidden_units, num_classes) # Output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, n_channels, n_timesteps)\n",
    "\n",
    "        # Convolution -> BatchNorm -> Activation\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # x shape: (batch_size, conv1_out_channels, conv1_out_timesteps)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x shape: (batch_size, fc1_input_features)\n",
    "\n",
    "        # Fully Connected Layers with ReLU and Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output Layer (Logits)\n",
    "        x = self.fc3(x)\n",
    "        # x shape: (batch_size, num_classes)\n",
    "        # Note: Softmax is typically applied outside the model, often in the loss function (e.g., CrossEntropyLoss)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_LE7IO3K0jy"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dqR8q61MK0jy"
   },
   "outputs": [],
   "source": [
    "# --- Modified Generic Training Function with Validation ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic function to train and validate a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader or None): DataLoader for the validation data. If None, validation is skipped.\n",
    "        criterion (nn.Module): The loss function (e.g., nn.CrossEntropyLoss).\n",
    "        optimizer (Optimizer): The optimizer (e.g., optim.Adam).\n",
    "        num_epochs (int): Number of epochs to train for.\n",
    "        device (torch.device): The device to train on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        None: Prints training and validation progress information directly.\n",
    "    \"\"\"\n",
    "    model.to(device) # Move model to the designated device\n",
    "    total_train_steps = len(train_loader)\n",
    "    if val_loader:\n",
    "        total_val_steps = len(val_loader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model.__class__.__name__} ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set the model to training mode\n",
    "        epoch_train_loss = 0.0\n",
    "        train_correct_predictions = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training statistics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total_samples += labels.size(0)\n",
    "            train_correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / total_train_steps\n",
    "        epoch_train_accuracy = 100 * train_correct_predictions / train_total_samples\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is not None:\n",
    "            model.eval() # Set the model to evaluation mode\n",
    "            epoch_val_loss = 0.0\n",
    "            val_correct_predictions = 0\n",
    "            val_total_samples = 0\n",
    "\n",
    "            with torch.no_grad(): # Disable gradient calculations during validation\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    # Move data to the designated device\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss_batch = criterion(val_outputs, val_labels)\n",
    "\n",
    "                    # Accumulate validation statistics\n",
    "                    epoch_val_loss += val_loss_batch.item()\n",
    "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                    val_total_samples += val_labels.size(0)\n",
    "                    val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "            # Calculate average validation loss and accuracy for the epoch\n",
    "            avg_epoch_val_loss = epoch_val_loss / total_val_steps\n",
    "            epoch_val_accuracy = 100 * val_correct_predictions / val_total_samples\n",
    "\n",
    "            # Print combined epoch results\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%')\n",
    "        else:\n",
    "            # Print only training results if no validation loader is provided\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%')\n",
    "\n",
    "        # Note: model is already set back to train() mode at the start of the next epoch loop iteration\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished Training {model.__class__.__name__}. Total time: {end_time - start_time:.2f} seconds\")\n",
    "    # --- Consider saving the best model based on validation performance ---\n",
    "    # (Logic for tracking best val_accuracy/lowest val_loss and saving model state_dict would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    num_classes: int,\n",
    "    class_names: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model and print a left-aligned classification report\n",
    "    and confusion matrix in the exact format requested.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in data_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            outputs = model(Xb)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    # compute report string\n",
    "    report_str = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=list(range(num_classes)),\n",
    "        target_names=class_names,\n",
    "        digits=6,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # print left-aligned report\n",
    "    for line in report_str.splitlines():\n",
    "        print(line.lstrip())\n",
    "\n",
    "    # print confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z84oEIa-K0jz"
   },
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7GAKWO3K0jz"
   },
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MY7y9L_fmViM"
   },
   "outputs": [],
   "source": [
    "folder_path_val = './fixed_data_aug/validate'\n",
    "folder_path_test = './fixed_data_aug/test'\n",
    "folder_path_train = './fixed_data_aug/train'\n",
    "file_path_labels = './fixed_data_aug/labels.json'\n",
    "\n",
    "# Load test data and labels\n",
    "data_val = collect_eeg_data(folder_path_val)\n",
    "data_test = collect_eeg_data(folder_path_test)\n",
    "data_train = collect_eeg_data(folder_path_train)\n",
    "data_labels = read_json_dicts(file_path_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE0CFDcOK0j0"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we match the labels with the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Strip 'test/' or 'train/' prefix from data_labels.file_name\n",
    "data_labels['file_name'] = data_labels['file_name'].str.replace(r'^(test/|train/)', '', regex=True)\n",
    "\n",
    "# Build a mapping from base filename → label\n",
    "label_map = dict(zip(data_labels['file_name'], data_labels['label']))\n",
    "\n",
    "# 2. Create y_val and filter data_val\n",
    "y_val = {}\n",
    "for fn in list(data_val.keys()):\n",
    "    if fn in label_map:\n",
    "        y_val[fn] = label_map[fn]\n",
    "# keep only matched entries in data_val\n",
    "data_val = {fn: data_val[fn] for fn in y_val}\n",
    "\n",
    "# 3. Create y_test and filter data_test\n",
    "y_test = {}\n",
    "for fn in list(data_test.keys()):\n",
    "    if fn in label_map:\n",
    "        y_test[fn] = label_map[fn]\n",
    "data_test = {fn: data_test[fn] for fn in y_test}\n",
    "\n",
    "# 4. Create y_train and filter data_train, handling augmented filenames\n",
    "def get_base_filename(fn: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove augmentation suffix (_amplitude_scale, _noise, _time_shift) before '.set'.\n",
    "    E.g. 'sub-002_eeg_chunk_0_noise.set' → 'sub-002_eeg_chunk_0.set'\n",
    "    \"\"\"\n",
    "    return re.sub(r'_(amplitude_scale|noise|time_shift)(?=\\.set$)', '', fn)\n",
    "\n",
    "y_train = {}\n",
    "for fn in list(data_train.keys()):\n",
    "    base = get_base_filename(fn)\n",
    "    if base in label_map:\n",
    "        y_train[fn] = label_map[base]\n",
    "# keep only matched entries in data_train\n",
    "data_train = {fn: data_train[fn] for fn in y_train}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Minimal Preprocessing WITHOUT Bandpass Filter (Only Standardization) ---\n",
      "Processed 898 test samples; example shape: (19, 1425)\n",
      "Processed 877 validation samples; example shape: (19, 1425)\n",
      "Processed 10644 train samples; example shape: (19, 1425)\n"
     ]
    }
   ],
   "source": [
    "# Set sampling frequency and disable bandpass by using lowcut=0 and highcut=fs\n",
    "fs = 95\n",
    "\n",
    "print(\"\\n--- Applying Minimal Preprocessing WITHOUT Bandpass Filter (Only Standardization) ---\")\n",
    "\n",
    "# Process TEST set\n",
    "X_test_processed = []\n",
    "for fname, eeg in data_test.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_test_processed.append(processed)\n",
    "print(f\"Processed {len(X_test_processed)} test samples; example shape: {X_test_processed[0].shape}\")\n",
    "\n",
    "# Process VALIDATION set\n",
    "X_val_processed = []\n",
    "for fname, eeg in data_val.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_val_processed.append(processed)\n",
    "print(f\"Processed {len(X_val_processed)} validation samples; example shape: {X_val_processed[0].shape}\")\n",
    "\n",
    "# Process TRAIN set\n",
    "X_train_processed = []\n",
    "for fname, eeg in data_train.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_train_processed.append(processed)\n",
    "print(f\"Processed {len(X_train_processed)} train samples; example shape: {X_train_processed[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn datasets into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting processed data + labels into tensors & dataloaders ---\n",
      "X_train: torch.Size([10644, 19, 1425]), y_train: torch.Size([10644])\n",
      "X_val  : torch.Size([877, 19, 1425]), y_val  : torch.Size([877])\n",
      "X_test : torch.Size([898, 19, 1425]), y_test : torch.Size([898])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Converting processed data + labels into tensors & dataloaders ---\")\n",
    "\n",
    "# Helper ─ collect labels in the SAME iteration order as the list was built\n",
    "def build_xy_tensors(X_processed_list, y_dict):\n",
    "    \"\"\"\n",
    "    Align X and y by dict iteration order, encode y with LabelEncoder,\n",
    "    and return (X_tensor, y_tensor).\n",
    "    \"\"\"\n",
    "    if not X_processed_list:\n",
    "        raise ValueError(\"The processed X list is empty!\")\n",
    "\n",
    "    # Preserve insertion order of the dict (Python 3.7+ guarantees this)\n",
    "    filenames_order = list(y_dict.keys())\n",
    "\n",
    "    # Sanity check\n",
    "    if len(filenames_order) != len(X_processed_list):\n",
    "        raise RuntimeError(\n",
    "            f\"Mismatch: len(X)={len(X_processed_list)} vs len(y)={len(filenames_order)}. \"\n",
    "            \"Check data alignment.\"\n",
    "        )\n",
    "\n",
    "    # Build y list according to the same order we used when filling X_processed\n",
    "    y_list = [y_dict[fname] for fname in filenames_order]\n",
    "\n",
    "    # --- Label encoding ---\n",
    "    # For consistency across splits, fit once on the union of all labels\n",
    "    global _global_le  # use a single LabelEncoder instance\n",
    "    if '_global_le' not in globals():\n",
    "        _global_le = LabelEncoder()\n",
    "        _global_le.fit(\n",
    "            list(y_train.values()) +\n",
    "            list(y_val.values())   +\n",
    "            list(y_test.values())\n",
    "        )\n",
    "\n",
    "    y_encoded = _global_le.transform(y_list)\n",
    "\n",
    "    # --- Convert to tensors ---\n",
    "    X_np = np.array(X_processed_list)          # shape: (N, C, T)\n",
    "    y_np = np.array(y_encoded, dtype=np.int64) # shape: (N,)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_np).float()\n",
    "    y_tensor = torch.from_numpy(y_np)          # long by default if dtype not set\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Build tensors for each split\n",
    "X_train_tensor, y_train_tensor = build_xy_tensors(X_train_processed, y_train)\n",
    "\n",
    "# Validation split \n",
    "if X_val_processed:\n",
    "    X_val_tensor, y_val_tensor = build_xy_tensors(X_val_processed, y_val)\n",
    "else:\n",
    "    X_val_tensor = torch.empty((0, *X_train_tensor.shape[1:]), dtype=torch.float32)\n",
    "    y_val_tensor = torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "# Test split\n",
    "if X_test_processed:\n",
    "    X_test_tensor, y_test_tensor = build_xy_tensors(X_test_processed, y_test)\n",
    "else:\n",
    "    X_test_tensor = torch.empty((0, *X_train_tensor.shape[1:]), dtype=torch.float32)\n",
    "    y_test_tensor = torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "# Log shapes\n",
    "print(f\"X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}\")\n",
    "print(f\"X_val  : {X_val_tensor.shape}, y_val  : {y_val_tensor.shape}\")\n",
    "print(f\"X_test : {X_test_tensor.shape}, y_test : {y_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {\n",
    "    0: 0.7941,   # A\n",
    "    1: 0.8360,   # C\n",
    "    2: 1.8364    # F\n",
    "}\n",
    "\n",
    "\n",
    "# ---------- utility: full pipeline for one run ----------\n",
    "def train_pipeline(\n",
    "    X_train_tensor: torch.Tensor,\n",
    "    y_train_tensor: torch.Tensor,\n",
    "    X_val_tensor : Optional[torch.Tensor],\n",
    "    y_val_tensor : Optional[torch.Tensor],\n",
    "    X_test_tensor: Optional[torch.Tensor],\n",
    "    y_test_tensor: Optional[torch.Tensor],\n",
    "    *,\n",
    "    learning_rate : float = 1e-3,\n",
    "    batch_size    : int   = 16,\n",
    "    optimizer     : str   = \"Adam\",\n",
    "    class_weight  : Optional[Union[torch.Tensor, Dict[int, float]]] = class_weight,\n",
    "    dropout_rate  : float = 0.5,\n",
    "    num_epochs    : int   = 1,\n",
    "    weight_decay  : float = 1e-4,\n",
    "    device        : torch.device = torch.device(\"cpu\"),\n",
    "    verbose       : bool  = False\n",
    ") -> Tuple[nn.Module, Dict[str, DataLoader]]:\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train_tensor, y_train_tensor),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader, test_loader = None, None\n",
    "    if X_val_tensor is not None and X_val_tensor.shape[0] > 0:\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(X_val_tensor, y_val_tensor),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    if X_test_tensor is not None and X_test_tensor.shape[0] > 0:\n",
    "        test_loader = DataLoader(\n",
    "            TensorDataset(X_test_tensor, y_test_tensor),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    n_channels, n_timesteps = X_train_tensor.shape[1], X_train_tensor.shape[2]\n",
    "    num_classes = len(_global_le.classes_)\n",
    "    model = Amini_Adapted_CNN(\n",
    "        n_channels=n_channels,\n",
    "        n_timesteps=n_timesteps,\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    if class_weight is not None:\n",
    "        if isinstance(class_weight, dict):\n",
    "            cw_tensor = torch.tensor(\n",
    "                [class_weight.get(i, 1.0) for i in range(num_classes)],\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            cw_tensor = class_weight.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=cw_tensor)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"adam\":\n",
    "        optim_obj = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay    # L2 regularization\n",
    "        )\n",
    "    elif opt_name == \"adamw\":\n",
    "        optim_obj = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    elif opt_name == \"sgd\":\n",
    "        optim_obj = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer {optimizer}\")\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optim_obj,\n",
    "        num_epochs,\n",
    "        device\n",
    "    )\n",
    "    return model, {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize best trackers\n",
    "best_acc = 0.0\n",
    "best_config = None\n",
    "best_model = None\n",
    "best_model_path = './best_model.path'\n",
    "\n",
    "# Define search space matching the previous Ray Tune setup\n",
    "tune_space = {\n",
    "    \"learning_rate\": np.logspace(-4, -2, num=100),  # continuous log space\n",
    "    \"batch_size\":    [16, 24, 32, 40, 48, 56, 64],\n",
    "    \"optimizer\":     [\"Adam\", \"AdamW\", \"SGD\"],\n",
    "    \"dropout_rate\":  np.linspace(0.3, 0.6, num=100),\n",
    "    \"num_epochs\":    [100],\n",
    "    \"weight_decay\":  np.logspace(-3, -1, num=50)     # L2 regularization strength\n",
    "}\n",
    "\n",
    "# Sample 10 random combinations\n",
    "drawer = list(ParameterSampler(tune_space, n_iter=10, random_state=42))\n",
    "\n",
    "# Loop through each sampled config\n",
    "for idx, config in enumerate(drawer, 1):\n",
    "    print(f\"\\n=== Configuration {idx} ===\")\n",
    "    print(config)\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    lr        = config[\"learning_rate\"]\n",
    "    bs        = config[\"batch_size\"]\n",
    "    opt_name  = config[\"optimizer\"]\n",
    "    do_rate   = config[\"dropout_rate\"]\n",
    "    epochs    = config[\"num_epochs\"]\n",
    "    wd        = config[\"weight_decay\"]\n",
    "\n",
    "    # Train using the existing pipeline\n",
    "    model, loaders = train_pipeline(\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        X_val_tensor,   y_val_tensor,\n",
    "        X_test_tensor,  y_test_tensor,\n",
    "        learning_rate=lr,\n",
    "        batch_size=bs,\n",
    "        optimizer=opt_name,\n",
    "        dropout_rate=do_rate,\n",
    "        num_epochs=epochs,\n",
    "        weight_decay=wd,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loader = loaders[\"val\"]\n",
    "    if val_loader is not None:\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(model.conv1.weight.device), yb.to(model.conv1.weight.device)\n",
    "                out = model(Xb)\n",
    "                pred = torch.argmax(out, dim=1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "        val_acc = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save model if it achieves a new best accuracy\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_config = config\n",
    "            best_model = model\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"--> New best model saved! (acc={best_acc:.2f}%)\")\n",
    "\n",
    "# Print summary of best result\n",
    "print(f\"\\n==> Best Validation Accuracy: {best_acc:.2f}%\")\n",
    "print(\"Best Hyperparameters:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    recall  f1-score   support\n",
      "\n",
      "A   0.519737  0.445070  0.479514       355\n",
      "C   0.426415  0.530516  0.472803       213\n",
      "F   0.483766  0.482201  0.482982       309\n",
      "\n",
      "accuracy                       0.478905       877\n",
      "macro avg   0.476639  0.485929  0.478433       877\n",
      "weighted avg   0.484398  0.478905  0.479106       877\n",
      "Confusion Matrix:\n",
      "[[158  79 118]\n",
      " [ 59 113  41]\n",
      " [ 87  73 149]]\n"
     ]
    }
   ],
   "source": [
    "report_df = evaluate_model(\n",
    "    model=best_model,\n",
    "    data_loader=val_loader,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    num_classes=len(_global_le.classes_),\n",
    "    class_names=_global_le.classes_.tolist()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
